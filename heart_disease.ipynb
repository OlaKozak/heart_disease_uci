{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import graphviz\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns:\n",
    "* age - age in years\n",
    "* sex - (1 = male; 0 = female)\n",
    "* cp - chest pain type\n",
    "* trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "* chol - serum cholestoral in mg/dl\n",
    "* fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "* restecg - resting electrocardiographic results\n",
    "* thalach - maximum heart rate achieved\n",
    "* exang - exercise induced angina (1 = yes; 0 = no)\n",
    "* oldpeak - ST depression induced by exercise relative to rest\n",
    "* slope - the slope of the peak exercise ST segment\n",
    "* ca - number of major vessels (0-3) colored by flourosopy\n",
    "* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "* target - 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "columns = [\"age\", \"sex\", \"cp\", \n",
    "           \"trestbps\", \"chol\", \"fbs\", \n",
    "           \"restecg\", \"thalach\",\"exang\", \n",
    "           \"oldpeak\",\"slope\", \"ca\", \"thal\", \"num\"]\n",
    "\n",
    "data_pd = pd.read_csv(path, header=None, names=columns, na_values=['?'])\n",
    "data_pd['target'] = np.where(data_pd.num > 0, 1, 0)\n",
    "data_pd.drop('num', inplace=True, axis=1)\n",
    "\n",
    "data_pd = data_pd.dropna() # 4 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Training (train/test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_pd[data_pd.columns[:-1]]\n",
    "y = data_pd['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) #default test split is 0.25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "modelCV = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "results = cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring) \n",
    "print(\"5-fold cross validation average accuracy: %.3f\" % (results.mean()))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "     'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), \n",
    "                   parameters, \n",
    "                   cv=5, \n",
    "                   scoring='accuracy', \n",
    "                   refit=True,\n",
    "                   return_train_score=True, \n",
    "                   n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "grid_logreg = grid_search.best_estimator_\n",
    "y_pred = grid_logreg.predict(X_test)\n",
    "y_proba = grid_logreg.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a classification report and confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['no heart disease', 'heart disease']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "fig.colorbar(im, ax=ax)\n",
    "#fig.colorbar(im)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw a precision-recall curve for the last model, this will show a trade-off between those two\n",
    "Precision - of those classified as having a heart disease, what proportion actually were having a heart disease (true positive rate)?\n",
    "Recall - of those actually having a heart disease how many were classified this way (positive predictive value)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(y_test, y_proba)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now draw a ROC curve. It's different from precision-recall plot as it plots a true positive rate vs false positive rate at different probability thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "print('AUC: %.3f' % auc_score)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(grid_logreg, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = data_pd.columns.tolist()[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(dt, X, y):\n",
    "    y_pred = dt.predict(X)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y, y_pred)\n",
    "    return auc(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Metrics on TRAIN SET:')\n",
    "y_pred = dt.predict(X_train)\n",
    "print('Accuracy of Decision Tree classifier on train set: {:.2f}'.format(dt.score(X_train, y_train)))\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "\n",
    "print('\\nMetrics on TEST SET:')\n",
    "y_pred = dt.predict(X_test)\n",
    "print('Accuracy of Decision Treeclassifier on test set: {:.2f}'.format(dt.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree visuallization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(dt, feature_names=data_pd.columns[:-1],  \n",
    "                                out_file=None, class_names=True,\n",
    "                                filled=True, rounded=True)  \n",
    "graphviz.Source(dot_data)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing results between train and test sets we can notice that decision tree is overfitting. It has learnt training set perfectly, but performs poorly on the test set. One possible explanation could be that the model is too complicated and it fits the training data too well. \n",
    "\n",
    "Lets try to validate that by manipulating tree parameters.\n",
    "\n",
    "Among others some important ones are:\n",
    "* **max_depth** (default=None) - The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* **min_samples_split** (default=2) - The minimum number of samples required to split an internal node (number or fraction of all samples)\n",
    "* **min_samples_leaf** (default=1) - The minimum number of samples required to be at a leaf node (number or fraction)\n",
    "* **max_features** (default=None) - The number of features to consider when looking for the best split (number, fraction, sqrt or log2).\n",
    "\n",
    "\n",
    "Lets see what's the effect of manipulating each of those parameters seprately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_auc(dt, X, y):\n",
    "    y_pred = dt.predict(X)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y, y_pred)\n",
    "    return auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "def plot_parameter_search(X_train, y_train, X_test, y_test, param_values, param_name):\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "\n",
    "    for val in param_values:\n",
    "        dt = tree.DecisionTreeClassifier(**{param_name: val})\n",
    "        dt.fit(X_train, y_train)\n",
    "\n",
    "        train_results.append(get_auc(dt, X_train, y_train))\n",
    "        test_results.append(get_auc(dt, X_test, y_test))\n",
    "\n",
    "    line1, = plt.plot(param_values, train_results, 'b', label='Train AUC')\n",
    "    line2, = plt.plot(param_values, test_results, 'r', label='Test AUC')\n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "    plt.ylabel('AUC score')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': np.linspace(1, 10, 10, endpoint=True),\n",
    "    'min_samples_leaf': np.linspace(0.01, 0.5, 20, endpoint=True),\n",
    "    'min_samples_split': np.linspace(0.0001, 1.0, 20, endpoint=True),\n",
    "    'max_features': list(range(1,X_train.shape[1])),\n",
    "    \n",
    "}\n",
    "    \n",
    "for param, values in parameters.items():\n",
    "    plot_parameter_search(X_train, y_train, X_test, y_test, values, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But can we look at this plots separately to pick the best parameters?\n",
    "\n",
    "Lets see what happens when we repeat max_features plot with different max_depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = list(range(1,X_train.shape[1]))\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for val in max_features:\n",
    "    dt = tree.DecisionTreeClassifier(**{'max_features': val, 'max_depth': 4})\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    train_results.append(get_auc(dt, X_train, y_train))\n",
    "    test_results.append(get_auc(dt, X_test, y_test))\n",
    "\n",
    "line1, = plt.plot(max_features, train_results, 'b', label='Train AUC')\n",
    "line2, = plt.plot(max_features, test_results, 'r', label='Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('max_features (max_depth=4)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why we need to check all possible combinations of parameters with **Grid Search**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(tree.DecisionTreeClassifier(), \n",
    "                   parameters, \n",
    "                   cv=5, \n",
    "                   scoring='roc_auc', \n",
    "                   refit=True,\n",
    "                   return_train_score=True, \n",
    "                   n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics on TRAIN SET:')\n",
    "y_pred = clf.predict(X_train)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "\n",
    "print('\\nMetrics on TEST SET:')\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, feature_names=data_pd.columns[:-1],  \n",
    "                                out_file=None, class_names=True,\n",
    "                                filled=True, rounded=True)  \n",
    "graphviz.Source(dot_data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are imporant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=clf.feature_importances_, y=data_pd.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(clf, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = data_pd.columns.tolist()[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced interpretation techniques:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
