{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import graphviz\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.fixes import signature\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns:\n",
    "* age - age in years\n",
    "* sex - (1 = male; 0 = female)\n",
    "* cp - chest pain type\n",
    "* trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "* chol - serum cholestoral in mg/dl\n",
    "* fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "* restecg - resting electrocardiographic results\n",
    "* thalach - maximum heart rate achieved\n",
    "* exang - exercise induced angina (1 = yes; 0 = no)\n",
    "* oldpeak - ST depression induced by exercise relative to rest\n",
    "* slope - the slope of the peak exercise ST segment\n",
    "* ca - number of major vessels (0-3) colored by flourosopy\n",
    "* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect, probably thalassemia \n",
    "* target - 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "columns = [\"age\", \"sex\", \"cp\", \n",
    "           \"trestbps\", \"chol\", \"fbs\", \n",
    "           \"restecg\", \"thalach\",\"exang\", \n",
    "           \"oldpeak\",\"slope\", \"ca\", \"thal\", \"num\"]\n",
    "\n",
    "data_pd = pd.read_csv(path, header=None, names=columns, na_values=['?'])\n",
    "data_pd['target'] = np.where(data_pd.num > 0, 1, 0)\n",
    "data_pd.drop('num', inplace=True, axis=1)\n",
    "\n",
    "data_pd = data_pd.dropna() # 4 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Training (train/test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_pd[data_pd.columns[:-1]]\n",
    "y = data_pd['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) #default test split is 0.25 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic logistic regression model - with all features and with default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_train)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(logreg.score(X_train, y_train)))\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper left corner of the confusion matrix - how many true 0s were identified as 0s (true negative). The lower right corner - how many of true 1s were identified as 1s (true positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation - better way to test your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"https://3gp10c1vpy442j63me73gy3s-wpengine.netdna-ssl.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-21-at-4.26.53-PM.png\"\n",
    "Image(url= path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "modelCV = LogisticRegression()\n",
    "scoring = 'recall'\n",
    "results = cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring) \n",
    "print(\"5-fold cross validation average recall: %.3f\" % (results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search - run multiple models with different (predefined) hyperparameters to obtain the model with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "     'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), \n",
    "                   parameters, \n",
    "                   cv=5, \n",
    "                   scoring='recall', \n",
    "                   refit=True,\n",
    "                   return_train_score=True, \n",
    "                   n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "grid_logreg = grid_search.best_estimator_\n",
    "y_pred = grid_logreg.predict(X_test)\n",
    "y_proba = grid_logreg.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a classification report and confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's plot the confusion matrix - with normalization and without. As classes are not equal, the normalization will allow for more robust analysis of what's going on in the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code can be found here https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (6,6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "class_names =['no heart disease', 'heart disease']\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's draw a precision-recall curve for the last model, this will show a trade-off between those two\n",
    "#### Precision - of those classified as having a heart disease, what proportion actually were having a heart disease (positive predictive value)\n",
    "#### Recall - of those actually having a heart disease how many were classified this way (true positive rate, 1 - false negative rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\"\n",
    "Image(url= path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision - recall trade-off.  If you have to recall everything, you will have to keep generating results which are not accurate, hence lowering your precision. In most extreme case you can build a model that will always predict 1s and your recall will be 100%, but your precision will be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(y_test, y_proba)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now draw a ROC curve. It's different from precision-recall plot as it plots a true positive rate vs false positive rate at different probability thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "print('AUC: %.3f' % auc_score)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = zip(grid_logreg.coef_[0], data_pd.columns.tolist()[:-1])\n",
    "coefs = np.array(sorted(coefs, key=lambda x: x[0], reverse=True))\n",
    "\n",
    "sns.barplot(x=coefs[:, 0].astype(float), y=coefs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odds ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = zip(np.exp(grid_logreg.coef_[0]), data_pd.columns.tolist()[:-1])\n",
    "coefs = np.array(sorted(coefs, key=lambda x: x[0], reverse=True))\n",
    "\n",
    "sns.barplot(x=coefs[:, 0].astype(float), y=coefs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another approach to feature importance - permutation tests! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(grid_logreg, n_iter = 50).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = data_pd.columns.tolist()[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(dt, X, y):\n",
    "    y_pred = dt.predict(X)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y, y_pred)\n",
    "    return auc(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Metrics on TRAIN SET:')\n",
    "y_pred = dt.predict(X_train)\n",
    "print('Accuracy of Decision Tree classifier on train set: {:.2f}'.format(dt.score(X_train, y_train)))\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "\n",
    "print('\\nMetrics on TEST SET:')\n",
    "y_pred = dt.predict(X_test)\n",
    "print('Accuracy of Decision Treeclassifier on test set: {:.2f}'.format(dt.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree visuallization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(dt, feature_names=data_pd.columns[:-1],  \n",
    "                                out_file=None, class_names=True,\n",
    "                                filled=True, rounded=True)  \n",
    "graphviz.Source(dot_data)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing results between train and test sets we can notice that decision tree is overfitting. It has learnt training set perfectly, but performs poorly on the test set. One possible explanation could be that the model is too complicated and it fits the training data too well. \n",
    "\n",
    "Lets try to validate that by manipulating tree parameters.\n",
    "\n",
    "Among others some important ones are:\n",
    "* **max_depth** (default=None) - The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* **min_samples_split** (default=2) - The minimum number of samples required to split an internal node (number or fraction of all samples)\n",
    "* **min_samples_leaf** (default=1) - The minimum number of samples required to be at a leaf node (number or fraction)\n",
    "* **max_features** (default=None) - The number of features to consider when looking for the best split (number, fraction, sqrt or log2).\n",
    "\n",
    "\n",
    "Lets see what's the effect of manipulating each of those parameters seprately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_auc(dt, X, y):\n",
    "    y_pred = dt.predict(X)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y, y_pred)\n",
    "    return auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "def plot_parameter_search(X_train, y_train, X_test, y_test, param_values, param_name):\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "\n",
    "    for val in param_values:\n",
    "        dt = tree.DecisionTreeClassifier(**{param_name: val})\n",
    "        dt.fit(X_train, y_train)\n",
    "\n",
    "        train_results.append(get_auc(dt, X_train, y_train))\n",
    "        test_results.append(get_auc(dt, X_test, y_test))\n",
    "\n",
    "    line1, = plt.plot(param_values, train_results, 'b', label='Train AUC')\n",
    "    line2, = plt.plot(param_values, test_results, 'r', label='Test AUC')\n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "    plt.ylabel('AUC score')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': np.linspace(1, 10, 20, endpoint=True),\n",
    "    'min_samples_leaf': np.linspace(0.01, 0.5, 10, endpoint=True),\n",
    "    'min_samples_split': np.linspace(0.0001, 1.0, 10, endpoint=True),\n",
    "    'max_features': list(range(1, int(X_train.shape[1] / 2))),\n",
    "    \n",
    "}\n",
    "    \n",
    "for param, values in parameters.items():\n",
    "    plot_parameter_search(X_train, y_train, X_test, y_test, values, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But can we look at this plots separately to pick the best parameters?\n",
    "\n",
    "Lets see what happens when we repeat max_features plot with different max_depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for val in parameters['max_features']:\n",
    "    dt = tree.DecisionTreeClassifier(**{'max_features': val, 'max_depth': 4})\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    train_results.append(get_auc(dt, X_train, y_train))\n",
    "    test_results.append(get_auc(dt, X_test, y_test))\n",
    "\n",
    "line1, = plt.plot(parameters['max_features'], train_results, 'b', label='Train AUC')\n",
    "line2, = plt.plot(parameters['max_features'], test_results, 'r', label='Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('max_features (max_depth=4)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why we need to check all possible combinations of parameters with **Grid Search**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(tree.DecisionTreeClassifier(), \n",
    "                   parameters, \n",
    "                   cv=5, \n",
    "                   scoring='recall', \n",
    "                   refit=True,\n",
    "                   return_train_score=True, \n",
    "                   n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics on TRAIN SET:')\n",
    "y_pred = clf.predict(X_train)\n",
    "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print(classification_report(y_train, y_pred))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "\n",
    "print('\\nMetrics on TEST SET:')\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, feature_names=data_pd.columns[:-1],  \n",
    "                                out_file=None, class_names=True,\n",
    "                                filled=True, rounded=True)  \n",
    "graphviz.Source(dot_data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are imporant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = zip(clf.feature_importances_, data_pd.columns.tolist()[:-1])\n",
    "coefs = np.array(sorted(coefs, key=lambda x: x[0], reverse=True))\n",
    "\n",
    "sns.barplot(x=coefs[:, 0].astype(float), y=coefs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(clf, n_iter = 50).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = data_pd.columns.tolist()[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
